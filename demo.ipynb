{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from ipywebrtc import CameraStream, ImageRecorder, VideoRecorder\n",
    "from imageio import v3 as iio\n",
    "from IPython.display import Image, Video\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record a Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CameraStream(constraints={'facing_mode': 'user', 'audio': False, 'video': {'width': 640, 'height': 480}})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "camera = CameraStream(constraints=\n",
    "                      {\"facing_mode\": \"user\",\n",
    "                       \"audio\": False,\n",
    "                       \"video\": { \"width\": 640, \"height\": 480 }\n",
    "                       })\n",
    "camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be04b079157f415ab0c35e074b8be653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VideoRecorder(stream=CameraStream(constraints={'facing_mode': 'user', 'audio': False, 'video': {'width': 640, â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recorder = VideoRecorder(stream=camera)\n",
    "recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 480, 640, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Frames\n",
    "with iio.imopen(recorder.video.value, \"r\", format=\"ffmpeg\") as video_file:\n",
    "    frames = video_file.read()\n",
    "frames.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get extract hand via mediapipe\n",
    "def getHandCoordinates(image):\n",
    "    \n",
    "    handsModule = mp.solutions.hands\n",
    "    drawingModule = mp.solutions.drawing_utils\n",
    "    \n",
    "    with handsModule.Hands(static_image_mode=True) as hands:\n",
    "        results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        h,w,c = image.shape\n",
    "        x_landmarks = []\n",
    "        y_landmarks = []\n",
    "        \n",
    "        # In case no hand is detected\n",
    "        if results.multi_hand_landmarks == None:\n",
    "            return -1,-1,-1,-1\n",
    "        \n",
    "        # In case hand is detected\n",
    "        for handLandmarks in results.multi_hand_landmarks:\n",
    "            for landmark in handLandmarks.landmark:\n",
    "                x_landmarks.append(landmark.x)\n",
    "                y_landmarks.append(landmark.y)\n",
    "                \n",
    "    # Add 5% padding to the hand\n",
    "    min_x = int(min(x_landmarks)*w*0.95)\n",
    "    min_y = int(min(y_landmarks)*h*0.95)\n",
    "    max_x = int(max(x_landmarks)*w*1.05)\n",
    "    max_y = int(max(y_landmarks)*h*1.05)\n",
    "    \n",
    "    # returns the coordinates of the hand + padding\n",
    "    return min_x, min_y, max_x, max_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Labelencoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.classes_ = np.load(\"labelencoder.npy\", allow_pickle=True)\n",
    "\n",
    "# Load Model\n",
    "model = tf.keras.models.load_model('./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_video = []\n",
    "\n",
    "face_detect = cv2.CascadeClassifier('haarcascade_frontalface_alt.xml')\n",
    "\n",
    "\n",
    "for frame in frames:\n",
    "    frame_conv = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    min_x, min_y, max_x, max_y = getHandCoordinates(frame_conv)\n",
    "    \n",
    "    if min_x == -1:\n",
    "        new_video.append(frame)\n",
    "        continue\n",
    "    \n",
    "    hand_frame = frame[min_y:max_y,min_x:max_x]\n",
    "    hand_frame = cv2.resize(hand_frame,dsize=(32,32))\n",
    "    \n",
    "    # Predict output\n",
    "    hand_tensor = tf.convert_to_tensor([hand_frame])\n",
    "    output = model.predict(hand_tensor)\n",
    "    letter = le.inverse_transform([tf.argmax(output, axis=1)[0].numpy()])[0]\n",
    "    \n",
    "    # Blur face\n",
    "    face_data = face_detect.detectMultiScale(frame, 1.3, 5)\n",
    "    for (x, y, w, h) in face_data:\n",
    "        roi = frame[y:y+h, x:x+w]\n",
    "        # applying a gaussian blur over this new rectangle area\n",
    "        roi = cv2.GaussianBlur(roi, (23, 23), 30)\n",
    "        # impose this blurred image on original image to get final image\n",
    "        frame[y:y+roi.shape[0], x:x+roi.shape[1]] = roi\n",
    "    \n",
    "    # Draw Rectangle \n",
    "    cv2.rectangle(frame, (min_x, min_y), (max_x, max_y), (255, 0, 0), 2)\n",
    "    cv2.putText(frame, letter[-1], (50, 50), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                   1, (255, 0, 0), 4, cv2.LINE_AA)\n",
    "    new_video.append(frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = np.stack(new_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"output.mov\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iio.imwrite(\"output.mov\", frames, fps=15)\n",
    "Video(\"output.mov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
